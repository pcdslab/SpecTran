{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_461450/856433031.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata_processing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhdf5_with_filtering\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAnnotatedSpectrumIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_processing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_dataloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizerDataModule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import tempfile\n",
    "import uuid\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "from data_processing.hdf5_with_filtering import AnnotatedSpectrumIndex\n",
    "from data_processing.tokenization_dataloader import TokenizerDataModule\n",
    "\n",
    "# Generate the vocabulary and save it to an output file.\n",
    "# The vocabulary is ordered from most frequent to least frequent, and contains all possible\n",
    "# \"single\" m/z values (including those not present in the training data), as well as the top n pairs of co-occurring peaks\n",
    "# \n",
    "# Note that this does not contain special characters (pad, mask, etc); these must be initialized\n",
    "# when loading with the vocabulary class\n",
    "\n",
    "# This is partially based on the data loading code from Casanovo: https://github.com/Noble-Lab/casanovo/blob/main/casanovo/denovo/model_runner.py\n",
    "\n",
    "\n",
    "# Discretizes the original floating-point m/z value \n",
    "# based on the token size, and rounds to an integer.\n",
    "\n",
    "# The token size is assumed to start at the integer value,\n",
    "# so the possible tokens are min_mz + token_ix * token_size\n",
    "\n",
    "# each original m/z value is rounded to the nearest possible token based on the token size\n",
    "\n",
    "# e.g. if min_mz is 50 and token_size = 0.2, the possible tokens are 50.0, 50.2, 50.4, ...\n",
    "# while if min_mz = 50 and token_size = 1, the possible tokens are 50, 51, 52, ...\n",
    "# To simplify logic dealing with tokens, values are multiplied by 10^decimal places so they are always represented as integers\n",
    "\n",
    "# For example, for an m/z value of 158.29474:\n",
    "# token_size = 1 -> 158\n",
    "# token_size = 0.1 -> 1583\n",
    "# token_size = 0.2 -> 1582\n",
    "# token_size = 0.5 -> 1585\n",
    "\n",
    "# Token size is passed as a string to avoid floating-point arithmetic errors,\n",
    "# as the output depends on the number of decimal places. It must be passed as a string with at least \n",
    "# one value after the decimal place (e.g. to have a token size of 1, str_token_size should be 1.0)\n",
    "def to_token(mz_float, str_token_size):\n",
    "    token_size = float(str_token_size)\n",
    "\n",
    "    discretized_mz = round(mz_float / token_size) * token_size\n",
    "\n",
    "    num_decimal_places = len(str_token_size.split(\".\")[1].rstrip(\"0\"))\n",
    "\n",
    "    return int(discretized_mz * pow(10, num_decimal_places))\n",
    "\n",
    "# Create the data loader to use for generating the vocabulary\n",
    "def create_tokenization_dataloader(tmp_dir, data_path, max_charge, batch_size, start_spec_ix, end_spec_ix, max_spectra_per_peptide, peaks_per_spectrum, min_mz, max_mz, min_intensity, remove_precursor_tol, random_seed, train_ratio):\n",
    "    train_filenames = glob(os.path.join(data_path, '*'))\n",
    "\n",
    "    data_idx_fn = os.path.join(tmp_dir.name, f\"{uuid.uuid4().hex}.hdf5\")\n",
    "\n",
    "    valid_charge = np.arange(1, max_charge + 1)\n",
    "    data_index = AnnotatedSpectrumIndex(\n",
    "        data_idx_fn, start_spec_ix=start_spec_ix, end_spec_ix=end_spec_ix, max_spectra_per_peptide=max_spectra_per_peptide, ms_data_files=train_filenames, valid_charge=valid_charge\n",
    "    )\n",
    "\n",
    "    dataloader_params = dict(\n",
    "        batch_size=batch_size,\n",
    "        n_peaks=peaks_per_spectrum,\n",
    "        min_mz=min_mz,\n",
    "        max_mz=max_mz,\n",
    "        min_intensity=min_intensity,\n",
    "        remove_precursor_tol=remove_precursor_tol,\n",
    "        random_seed = random_seed,\n",
    "        train_ratio = train_ratio\n",
    "    )\n",
    "\n",
    "    data_loader = TokenizerDataModule(data_index=data_index, **dataloader_params)\n",
    "\n",
    "    data_loader.setup()\n",
    "\n",
    "    return data_loader\n",
    "\n",
    "# Calculate the frequencies of all single tokens and pair tokens \n",
    "# using the provided dataloader. \n",
    "\n",
    "# token_size should be a string indicating the granularity of the tokens to return.\n",
    "# For example, if the m/z values should be rounded to the nearest 0.2 increments, token_size should be \"0.2\", or for 1.0 increments, \"1.0\"\n",
    "def calculate_single_and_pair_token_frequencies(train_data_loader, min_mz, max_mz, token_size):\n",
    "    # first, calculate the \"raw\" single token frequencies and frequencies for all pairs\n",
    "    single_token_frequencies = {}\n",
    "    for mz_token in np.arange(min_mz, max_mz, float(token_size)):\n",
    "        single_token_frequencies[to_token(mz_token, token_size)] = 0\n",
    "\n",
    "    combination_token_frequencies = defaultdict(lambda:0)    \n",
    "\n",
    "    for batch in train_data_loader:\n",
    "        spectra = batch[0]\n",
    "        for spec in spectra:\n",
    "\n",
    "            # for each discretized m/z value (token) in the spectrum, keep the highest intensity \n",
    "            # (if multiple peaks fall into the same \"bin\", we consider only the one with the higest intensity)\n",
    "            peaks_for_this_spec = defaultdict(lambda:0)\n",
    "\n",
    "            for peak in spec:\n",
    "                if peak[0] == 0:\n",
    "                    break # we've reached the end padding, so no need to continue processing this spectrum\n",
    "\n",
    "                mz_token = to_token(peak[0].item(), token_size)\n",
    "                intensity = peak[1]\n",
    "\n",
    "                previous_intensity = peaks_for_this_spec[mz_token]\n",
    "\n",
    "                peaks_for_this_spec[mz_token] = max(intensity, previous_intensity)\n",
    "\n",
    "            # store the frequency of all single peaks for this spectrum in the overall dict\n",
    "            for peak in peaks_for_this_spec.keys():\n",
    "                single_token_frequencies[peak] += 1\n",
    "\n",
    "            combinations_in_this_spec = itertools.combinations(peaks_for_this_spec.keys(), 2)\n",
    "\n",
    "            for peak_combination in combinations_in_this_spec:\n",
    "                combination_token_frequencies[peak_combination] += 1\n",
    "\n",
    "    return single_token_frequencies, combination_token_frequencies         \n",
    "\n",
    "    # generate the full vocabularies from the frequencies, and save to the output files.\n",
    "    # this saves two vocabularies: one containing just the single tokens, and one containing\n",
    "    # all single tokens as well as the top pair_token_count pair tokens.\n",
    "\n",
    "    # Each line in the file is a tuple containing the token itself (either a single or pair of m/z values), and its frequency\n",
    "\n",
    "    # the vocabulary files are sorted by frequency (highest first).\n",
    "    # For single-only vocabularies, the frequency is simply the number of times that token appeared in the training data.\n",
    "    # For the vocabulary including pairs, the frequency of single tokens only considers their frequency when they appear on their own (but not as part of the top pair_token_count pair tokens)\n",
    "def generate_and_save_vocabulary(train_data_loader, single_token_frequencies, pair_token_frequencies, token_size, single_output_file_name, pair_token_count=0, pair_output_file_name=None):\n",
    "\n",
    "    # first save the singles-only vocabulary\n",
    "    sorted_single_frequencies = sorted(single_token_frequencies.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    with open(single_output_file_name, \"w\") as f:\n",
    "        for line in sorted_single_frequencies:\n",
    "            f.write(f\"{line}\\n\")\n",
    "\n",
    "    # now find the top pair_token_count pairs, update the frequency of their respective singles, and save to the pair vocabulary file\n",
    "    sorted_pair_frequencies = sorted(pair_token_frequencies.items(), key=lambda kv: kv[1], reverse=True)[:pair_token_count]\n",
    "\n",
    "    overall_frequencies = defaultdict(lambda:0)\n",
    "\n",
    "    # make sure we have tokens for all singles values even if they do not appear in the data\n",
    "    for key in single_token_frequencies.keys():\n",
    "        overall_frequencies[key] = 0\n",
    "\n",
    "\n",
    "    # preprocess the list of pair tokens into a map from each single m/z value to the pair tokens that start with it\n",
    "    mz_to_pairs_starting_with_it = defaultdict(lambda:set())\n",
    "    for pair_token in sorted_pair_frequencies:\n",
    "        token = pair_token[0] # the tuple representing the m/z pair, e.g. (244, 279)\n",
    "        first_mz = token[0]\n",
    "\n",
    "        mz_to_pairs_starting_with_it[first_mz].add(token)\n",
    "\n",
    "    for batch in train_data_loader:\n",
    "        spectra = batch[0]\n",
    "        for spec in spectra:\n",
    "            peaks_in_pairs_this_spec = set() # keep track of the peaks that are part of pair tokens so far, to prevent double counting\n",
    "\n",
    "            peaks_for_spec = set()\n",
    "            for peak in spec:\n",
    "                if peak[0] == 0:\n",
    "                    break # ignore padding\n",
    "                peaks_for_spec.add(to_token(peak[0].item(), token_size))\n",
    "\n",
    "            for peak in peaks_for_spec:\n",
    "                possible_tokens_starting_with_this_mz = mz_to_pairs_starting_with_it[peak]\n",
    "\n",
    "                for mz_pair in possible_tokens_starting_with_this_mz:\n",
    "                    second_mz = mz_pair[1]\n",
    "\n",
    "                    if second_mz in peaks_for_spec:\n",
    "                        overall_frequencies[mz_pair] += 1\n",
    "                        peaks_in_pairs_this_spec.add(peak)\n",
    "                        peaks_in_pairs_this_spec.add(second_mz)\n",
    "\n",
    "                if peak not in peaks_in_pairs_this_spec:\n",
    "                    # only increment the count for the single token if not part of any pairs \n",
    "                    overall_frequencies[peak] += 1\n",
    "\n",
    "    # now sort the overall frequencies and save to the file:\n",
    "    sorted_overall_frequencies = sorted(overall_frequencies.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    with open(pair_output_file_name, \"w\") as f:\n",
    "        for line in sorted_overall_frequencies:\n",
    "            f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "029c3652fc1f4b42ad604b91f5546b60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "/disk/dragon-storage/homes/seber007/data/spectra/massive-kb-lowres/LIBRARY_AUGMENT-20026e39-download_filtered_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp_dir = tempfile.TemporaryDirectory()\n",
    "\n",
    "data_loader = create_tokenization_dataloader(tmp_dir, data_path=\"/disk/dragon-storage/homes/seber007/data/spectra/massive-kb-lowres/\", max_charge=10, batch_size=1000, start_spec_ix=0, end_spec_ix=200_000, max_spectra_per_peptide=1, peaks_per_spectrum=150, min_mz=50.0, max_mz=2500.0, min_intensity=0.01, remove_precursor_tol=2.0, random_seed=42, train_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = data_loader.train_dataloader()\n",
    "val_dataloader = data_loader.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_token_frequencies, pair_token_frequencies = calculate_single_and_pair_token_frequencies(train_data_loader=train_dataloader, min_mz=50.0, max_mz=2500.0, token_size=\"1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_and_save_vocabulary(train_data_loader=train_dataloader, single_token_frequencies=single_token_frequencies, pair_token_frequencies=pair_token_frequencies, token_size=\"1.0\", single_output_file_name=\"/lclhome/seber007/single-tokens-mar22.txt\", pair_token_count=5550, pair_output_file_name=\"/lclhome/seber007/pair-tokens-mar22.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac97678c709df623c479ad32f8f18177f27e419f71d040cdac87793227800db1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
